{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "<b>Created by Aruna on 31/5/19</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aruna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aruna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "from nltk import corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Text cleaning functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1: Make list of tweets of string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeListOfTweets(A):\n",
    "    tw = []\n",
    "    \n",
    "    for item in A['tweet']:\n",
    "        tw.append(str(item))\n",
    "    \n",
    "    return tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Replace symbols like #, &amp; with whitespace and remove any hex characters or '@user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSymbols(A):\n",
    "    for i in range(0, len(A)):\n",
    "        A[i] = A[i].replace(\"@user\", \"\")\n",
    "        A[i] = A[i].replace(\"#\", \" \")\n",
    "        A[i] = A[i].replace(\"&amp;\", \" \")\n",
    "        A[i] = A[i].encode('ascii', errors='ignore').decode(\"utf-8\")\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #3: Tokenize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeTweets(A):\n",
    "    tokens = []\n",
    "    for t in A:\n",
    "        tokens.append(word_tokenize(t))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Load the training CSV data using pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"train.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>when a father is dysfunctional and is so sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks for  lyft credit i can't use cause th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now     motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>camping tomorrow        danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams. ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!!  allin  cavs  champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>welcome here !  i'm   it's so  gr8 !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>ireland consumer price index (mom) climbed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>we are so selfish.  orlando  standwithorlando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>i get to see my daddy today!!    80days  getti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>cnn calls  michigan middle school 'build the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in  australia    opkillingbay  se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch...junior is angry got7  junior  yugyoem  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>i am thankful for having a paner.  thankful  p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>its  friday!  smiles all around via ig user:  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>as we all know, essential oils are not made of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>euro2016 people blaming ha for conceded goal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>sad little dude..    badday  coneofshame  cats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>product of the day: happy man  wine tool  who'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>tgif    ff to my  gamedev  indiedev  indieg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>beautiful sign by vendor 80 for $45.00!!  upsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>all  smiles when  media is   !!   pressconfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>we had a great panel on the mediatization of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>happy father's day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>50 people went to nightclub to have a good nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31932</th>\n",
       "      <td>31933</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31933</th>\n",
       "      <td>31934</td>\n",
       "      <td>1</td>\n",
       "      <td>judd is a     homophobic  freemilo  milo  fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31934</th>\n",
       "      <td>31935</td>\n",
       "      <td>1</td>\n",
       "      <td>lady banned from kentucky mall.    jcpenny  ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31935</th>\n",
       "      <td>31936</td>\n",
       "      <td>0</td>\n",
       "      <td>ugh i'm trying to enjoy my happy hour drink   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31936</th>\n",
       "      <td>31937</td>\n",
       "      <td>0</td>\n",
       "      <td>want to know how to live a   life? do more thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31937</th>\n",
       "      <td>31938</td>\n",
       "      <td>0</td>\n",
       "      <td>love island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31938</th>\n",
       "      <td>31939</td>\n",
       "      <td>0</td>\n",
       "      <td>my fav actor  vijaysethupathi ! my fav actress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31939</th>\n",
       "      <td>31940</td>\n",
       "      <td>0</td>\n",
       "      <td>whew   it's a productive and    friday!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31940</th>\n",
       "      <td>31941</td>\n",
       "      <td>0</td>\n",
       "      <td>she's finally here!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31941</th>\n",
       "      <td>31942</td>\n",
       "      <td>0</td>\n",
       "      <td>passed first year of uni  yay  love  pass  uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31942</th>\n",
       "      <td>31943</td>\n",
       "      <td>0</td>\n",
       "      <td>this week is flying by    humpday -  wednesday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31943</th>\n",
       "      <td>31944</td>\n",
       "      <td>0</td>\n",
       "      <td>modeling photoshoot this friday yay  model  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31944</th>\n",
       "      <td>31945</td>\n",
       "      <td>0</td>\n",
       "      <td>you're surrounded by people who love you (even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31945</th>\n",
       "      <td>31946</td>\n",
       "      <td>0</td>\n",
       "      <td>feel like...   dog  summer  hot  help  sun  da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31946</th>\n",
       "      <td>31947</td>\n",
       "      <td>1</td>\n",
       "      <td>omfg i'm offended! i'm a  mailbox and i'm pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31947</th>\n",
       "      <td>31948</td>\n",
       "      <td>1</td>\n",
       "      <td>you don't have the balls to hashtag me as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31948</th>\n",
       "      <td>31949</td>\n",
       "      <td>1</td>\n",
       "      <td>makes you ask yourself, who am i? then am i a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31949</th>\n",
       "      <td>31950</td>\n",
       "      <td>0</td>\n",
       "      <td>hear one of my new songs! don't go - katie ell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31950</th>\n",
       "      <td>31951</td>\n",
       "      <td>0</td>\n",
       "      <td>you can try to 'tail' us to stop, 'butt' we'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31951</th>\n",
       "      <td>31952</td>\n",
       "      <td>0</td>\n",
       "      <td>i've just posted a new blog:  secondlife  lone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31952</th>\n",
       "      <td>31953</td>\n",
       "      <td>0</td>\n",
       "      <td>you went too far with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31953</th>\n",
       "      <td>31954</td>\n",
       "      <td>0</td>\n",
       "      <td>good morning  instagram  shower  water  berlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31954</th>\n",
       "      <td>31955</td>\n",
       "      <td>0</td>\n",
       "      <td>holiday   bull up: you will dominate your bul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31955</th>\n",
       "      <td>31956</td>\n",
       "      <td>0</td>\n",
       "      <td>less than 2 weeks    ibiza bringiton mallorca ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31956</th>\n",
       "      <td>31957</td>\n",
       "      <td>0</td>\n",
       "      <td>off fishing tomorrow  carnt wait first time in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0</td>\n",
       "      <td>ate  isz that youuu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>sikh  temple vandalised in in  calgary,  wso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0</td>\n",
       "      <td>thank you  for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "0          1      0    when a father is dysfunctional and is so sel...\n",
       "1          2      0    thanks for  lyft credit i can't use cause th...\n",
       "2          3      0                                bihday your majesty\n",
       "3          4      0   model   i love u take with u all the time in ...\n",
       "4          5      0             factsguide: society now     motivation\n",
       "5          6      0  [2/2] huge fan fare and big talking before the...\n",
       "6          7      0                      camping tomorrow        danny\n",
       "7          8      0  the next school year is the year for exams. ca...\n",
       "8          9      0  we won!!! love the land!!!  allin  cavs  champ...\n",
       "9         10      0              welcome here !  i'm   it's so  gr8 ! \n",
       "10        11      0     ireland consumer price index (mom) climbed ...\n",
       "11        12      0  we are so selfish.  orlando  standwithorlando ...\n",
       "12        13      0  i get to see my daddy today!!    80days  getti...\n",
       "13        14      1    cnn calls  michigan middle school 'build the...\n",
       "14        15      1  no comment!  in  australia    opkillingbay  se...\n",
       "15        16      0  ouch...junior is angry got7  junior  yugyoem  ...\n",
       "16        17      0  i am thankful for having a paner.  thankful  p...\n",
       "17        18      1                             retweet if you agree! \n",
       "18        19      0  its  friday!  smiles all around via ig user:  ...\n",
       "19        20      0  as we all know, essential oils are not made of...\n",
       "20        21      0   euro2016 people blaming ha for conceded goal ...\n",
       "21        22      0  sad little dude..    badday  coneofshame  cats...\n",
       "22        23      0  product of the day: happy man  wine tool  who'...\n",
       "23        24      1                lumpy says i am a . prove it lumpy.\n",
       "24        25      0     tgif    ff to my  gamedev  indiedev  indieg...\n",
       "25        26      0  beautiful sign by vendor 80 for $45.00!!  upsi...\n",
       "26        27      0    all  smiles when  media is   !!   pressconfe...\n",
       "27        28      0  we had a great panel on the mediatization of t...\n",
       "28        29      0                             happy father's day    \n",
       "29        30      0  50 people went to nightclub to have a good nig...\n",
       "...      ...    ...                                                ...\n",
       "31932  31933      0                                     thanks gemma  \n",
       "31933  31934      1   judd is a     homophobic  freemilo  milo  fre...\n",
       "31934  31935      1  lady banned from kentucky mall.    jcpenny  ke...\n",
       "31935  31936      0  ugh i'm trying to enjoy my happy hour drink   ...\n",
       "31936  31937      0  want to know how to live a   life? do more thi...\n",
       "31937  31938      0                                     love island   \n",
       "31938  31939      0  my fav actor  vijaysethupathi ! my fav actress...\n",
       "31939  31940      0          whew   it's a productive and    friday!!!\n",
       "31940  31941      0                            she's finally here!    \n",
       "31941  31942      0  passed first year of uni  yay  love  pass  uni...\n",
       "31942  31943      0  this week is flying by    humpday -  wednesday...\n",
       "31943  31944      0    modeling photoshoot this friday yay  model  ...\n",
       "31944  31945      0  you're surrounded by people who love you (even...\n",
       "31945  31946      0  feel like...   dog  summer  hot  help  sun  da...\n",
       "31946  31947      1   omfg i'm offended! i'm a  mailbox and i'm pro...\n",
       "31947  31948      1    you don't have the balls to hashtag me as a ...\n",
       "31948  31949      1   makes you ask yourself, who am i? then am i a...\n",
       "31949  31950      0  hear one of my new songs! don't go - katie ell...\n",
       "31950  31951      0    you can try to 'tail' us to stop, 'butt' we'...\n",
       "31951  31952      0  i've just posted a new blog:  secondlife  lone...\n",
       "31952  31953      0                           you went too far with   \n",
       "31953  31954      0  good morning  instagram  shower  water  berlin...\n",
       "31954  31955      0   holiday   bull up: you will dominate your bul...\n",
       "31955  31956      0  less than 2 weeks    ibiza bringiton mallorca ...\n",
       "31956  31957      0  off fishing tomorrow  carnt wait first time in...\n",
       "31957  31958      0                              ate  isz that youuu? \n",
       "31958  31959      0    to see nina turner on the airwaves trying to...\n",
       "31959  31960      0  listening to sad songs on a monday morning otw...\n",
       "31960  31961      1    sikh  temple vandalised in in  calgary,  wso...\n",
       "31961  31962      0                        thank you  for you follow  \n",
       "\n",
       "[31962 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = makeListOfTweets(tweets)\n",
    "tw = removeSymbols(tw)\n",
    "# tokens = tokenizeTweets(tw)\n",
    "\n",
    "tweets['tweet'] = tw\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Loading the test CSV data using pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"test_tweets.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>studiolife  aislife  requires  passion  dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>white  supremacists want everyone to see th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your  acne!!     altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd  bihday to my amazing, hilarious  nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31968</td>\n",
       "      <td>choose to be   :)  momtips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31969</td>\n",
       "      <td>something inside me dies   eyes ness  smokeyey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31970</td>\n",
       "      <td>finished tattoo inked ink loveit    thanks al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31971</td>\n",
       "      <td>i will never understand why my dad left me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31972</td>\n",
       "      <td>delicious    food  lovelife  capetown mannaep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31973</td>\n",
       "      <td>1000dayswasted - narcosis infinite ep.. make m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31974</td>\n",
       "      <td>one of the world's greatest spoing events    l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31975</td>\n",
       "      <td>half way through the website now and  allgoing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31976</td>\n",
       "      <td>good food, good life ,  enjoy and    this is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31977</td>\n",
       "      <td>i'll stand behind this  guncontrolplease    se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31978</td>\n",
       "      <td>i ate,i ate and i ate...    jamaisasthi  fish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31979</td>\n",
       "      <td>got my  limited edition rain or shine set to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31980</td>\n",
       "      <td>love    hugs    kisses too! how to keep you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31981</td>\n",
       "      <td>girls    sun  fave @ london, united kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31982</td>\n",
       "      <td>thought factory: bbc neutrality on right wing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31983</td>\n",
       "      <td>hey guys tommorow is the last day of my exams ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31984</td>\n",
       "      <td>levyrroni  recuerdos memories  recuerdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31985</td>\n",
       "      <td>my mind is like  but my body like ....    slee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31986</td>\n",
       "      <td>never been this down on myself in my entire li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31987</td>\n",
       "      <td>check  twitterww - trends: \"trending worldwide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31988</td>\n",
       "      <td>i thought i saw a mermaid!!!  ceegee   smcr   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31989</td>\n",
       "      <td>chick gets fucked hottest naked lady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31990</td>\n",
       "      <td>happy bday lucy xoxo  love  beautiful  pizza  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31991</td>\n",
       "      <td>haroldfriday have a weekend filled with sunbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31992</td>\n",
       "      <td>tried that! but nothing - will try again! kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17167</th>\n",
       "      <td>49130</td>\n",
       "      <td>people do anything for fucking attention nowad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17168</th>\n",
       "      <td>49131</td>\n",
       "      <td>creative bubble got burst  looking forward to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17169</th>\n",
       "      <td>49132</td>\n",
       "      <td>tomorrow is gonna be a big day! we are going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17170</th>\n",
       "      <td>49133</td>\n",
       "      <td>i am thankful for baby giggles.  thankful  pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17171</th>\n",
       "      <td>49134</td>\n",
       "      <td>model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17172</th>\n",
       "      <td>49135</td>\n",
       "      <td>in life u will grow to learn some pple will wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17173</th>\n",
       "      <td>49136</td>\n",
       "      <td>i was the storm,you were the rain. together we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17174</th>\n",
       "      <td>49137</td>\n",
       "      <td>lovelgq -  broken ep via    rnb  love  heabrok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>49138</td>\n",
       "      <td>spread love not hate  prayingfororlando    lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17176</th>\n",
       "      <td>49139</td>\n",
       "      <td>are the most racist pay ever!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17177</th>\n",
       "      <td>49140</td>\n",
       "      <td>i am thankful for children.  thankful  positiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17178</th>\n",
       "      <td>49141</td>\n",
       "      <td>liverpool   walk  liverpool  starbucks  avidae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17179</th>\n",
       "      <td>49142</td>\n",
       "      <td>bakersfield   rooster simulation: i want to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17180</th>\n",
       "      <td>49143</td>\n",
       "      <td>por do sol  instagood  beautiful    instadaily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17181</th>\n",
       "      <td>49144</td>\n",
       "      <td>hell yeah what a great surprise for your pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17182</th>\n",
       "      <td>49145</td>\n",
       "      <td>when ur the joke ur defensive towards everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17183</th>\n",
       "      <td>49146</td>\n",
       "      <td>enjoying the  evening  sun in my  bedroom    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17184</th>\n",
       "      <td>49147</td>\n",
       "      <td>tonight on  from 9pm gmt  you can here a speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17185</th>\n",
       "      <td>49148</td>\n",
       "      <td>today is a good day for excercise  imready  so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17186</th>\n",
       "      <td>49149</td>\n",
       "      <td>good night with a tea and music   billy  music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17187</th>\n",
       "      <td>49150</td>\n",
       "      <td>loving life   createyourfuture    lifestyle  h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17188</th>\n",
       "      <td>49151</td>\n",
       "      <td>black professor demonizes, proposes nazi style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17189</th>\n",
       "      <td>49152</td>\n",
       "      <td>learn how to think positive.   positive    ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17190</th>\n",
       "      <td>49153</td>\n",
       "      <td>we love the pretty, happy and fresh you!  teen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17191</th>\n",
       "      <td>49154</td>\n",
       "      <td>2_damn_tuff-ruff_muff__techno_city-(ng005)-web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>49155</td>\n",
       "      <td>thought factory: left-right polarisation!  tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17193</th>\n",
       "      <td>49156</td>\n",
       "      <td>feeling like a mermaid   hairflip  neverready ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17194</th>\n",
       "      <td>49157</td>\n",
       "      <td>hillary  campaigned today in  ohio((omg))   u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17195</th>\n",
       "      <td>49158</td>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17196</th>\n",
       "      <td>49159</td>\n",
       "      <td>my   song \"so glad\" free download!   shoegaze ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17197 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet\n",
       "0      31963   studiolife  aislife  requires  passion  dedic...\n",
       "1      31964     white  supremacists want everyone to see th...\n",
       "2      31965  safe ways to heal your  acne!!     altwaystohe...\n",
       "3      31966  is the hp and the cursed child book up for res...\n",
       "4      31967    3rd  bihday to my amazing, hilarious  nephew...\n",
       "5      31968                        choose to be   :)  momtips \n",
       "6      31969  something inside me dies   eyes ness  smokeyey...\n",
       "7      31970   finished tattoo inked ink loveit    thanks al...\n",
       "8      31971      i will never understand why my dad left me...\n",
       "9      31972   delicious    food  lovelife  capetown mannaep...\n",
       "10     31973  1000dayswasted - narcosis infinite ep.. make m...\n",
       "11     31974  one of the world's greatest spoing events    l...\n",
       "12     31975  half way through the website now and  allgoing...\n",
       "13     31976  good food, good life ,  enjoy and    this is c...\n",
       "14     31977  i'll stand behind this  guncontrolplease    se...\n",
       "15     31978  i ate,i ate and i ate...    jamaisasthi  fish ...\n",
       "16     31979    got my  limited edition rain or shine set to...\n",
       "17     31980     love    hugs    kisses too! how to keep you...\n",
       "18     31981       girls    sun  fave @ london, united kingdom \n",
       "19     31982  thought factory: bbc neutrality on right wing ...\n",
       "20     31983  hey guys tommorow is the last day of my exams ...\n",
       "21     31984         levyrroni  recuerdos memories  recuerdo...\n",
       "22     31985  my mind is like  but my body like ....    slee...\n",
       "23     31986  never been this down on myself in my entire li...\n",
       "24     31987  check  twitterww - trends: \"trending worldwide...\n",
       "25     31988  i thought i saw a mermaid!!!  ceegee   smcr   ...\n",
       "26     31989              chick gets fucked hottest naked lady \n",
       "27     31990  happy bday lucy xoxo  love  beautiful  pizza  ...\n",
       "28     31991   haroldfriday have a weekend filled with sunbe...\n",
       "29     31992    tried that! but nothing - will try again! kn...\n",
       "...      ...                                                ...\n",
       "17167  49130  people do anything for fucking attention nowad...\n",
       "17168  49131  creative bubble got burst  looking forward to ...\n",
       "17169  49132  tomorrow is gonna be a big day! we are going t...\n",
       "17170  49133  i am thankful for baby giggles.  thankful  pos...\n",
       "17171  49134   model   i love u take with u all the time in ...\n",
       "17172  49135  in life u will grow to learn some pple will wo...\n",
       "17173  49136  i was the storm,you were the rain. together we...\n",
       "17174  49137  lovelgq -  broken ep via    rnb  love  heabrok...\n",
       "17175  49138  spread love not hate  prayingfororlando    lov...\n",
       "17176  49139                 are the most racist pay ever!!!!! \n",
       "17177  49140  i am thankful for children.  thankful  positiv...\n",
       "17178  49141  liverpool   walk  liverpool  starbucks  avidae...\n",
       "17179  49142   bakersfield   rooster simulation: i want to c...\n",
       "17180  49143  por do sol  instagood  beautiful    instadaily...\n",
       "17181  49144   hell yeah what a great surprise for your pres...\n",
       "17182  49145  when ur the joke ur defensive towards everything \n",
       "17183  49146   enjoying the  evening  sun in my  bedroom    ...\n",
       "17184  49147  tonight on  from 9pm gmt  you can here a speci...\n",
       "17185  49148  today is a good day for excercise  imready  so...\n",
       "17186  49149  good night with a tea and music   billy  music...\n",
       "17187  49150  loving life   createyourfuture    lifestyle  h...\n",
       "17188  49151  black professor demonizes, proposes nazi style...\n",
       "17189  49152  learn how to think positive.   positive    ins...\n",
       "17190  49153  we love the pretty, happy and fresh you!  teen...\n",
       "17191  49154  2_damn_tuff-ruff_muff__techno_city-(ng005)-web...\n",
       "17192  49155  thought factory: left-right polarisation!  tru...\n",
       "17193  49156  feeling like a mermaid   hairflip  neverready ...\n",
       "17194  49157   hillary  campaigned today in  ohio((omg))   u...\n",
       "17195  49158  happy, at work conference: right mindset leads...\n",
       "17196  49159  my   song \"so glad\" free download!   shoegaze ...\n",
       "\n",
       "[17197 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = makeListOfTweets(test)\n",
    "tt = removeSymbols(tt)\n",
    "# testtokens = tokenizeTweets(tt)\n",
    "\n",
    "test['tweet'] = tt\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Now to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tweets['tweet']\n",
    "X_test = test['tweet']\n",
    "y_train = tweets['label']\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 39257)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('rfc',LinearSVC()),])\n",
    "\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['id'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Finally, store the model in a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_predictions.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['id', 'label'])\n",
    "    \n",
    "    for i in range(0, len(test['label'])):\n",
    "        writer.writerow([test['id'][i], test['label'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg = xgb.XGBClassifier(gamma=1,                 \n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    n_estimators=10000,                                                                    \n",
    "    subsample=0.8,\n",
    "    random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=1, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=10000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=34, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('rfc', XGBClassifier()),])\n",
    "\n",
    "text_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can not initialize DMatrix from Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m                 \u001b[0mcsr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-da79298f4205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \"\"\"\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[0mtest_dmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"best_ntree_limit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m                 raise TypeError('can not initialize DMatrix from'\n\u001b[1;32m--> 372\u001b[1;33m                                 ' {}'.format(type(data).__name__))\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can not initialize DMatrix from Series"
     ]
    }
   ],
   "source": [
    "predictions = xg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
